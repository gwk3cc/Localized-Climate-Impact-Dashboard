{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf84a338-5d0f-48e2-8805-473b0fd01178",
   "metadata": {},
   "source": [
    "# Climate Risk and Impact Assessment Dashboard for US Counties\n",
    "\n",
    "## Documentation\n",
    "\n",
    "### Python Files Required\n",
    "<b>climate_risk_webscrape.py</b>: This script automates the process of scraping climate risk data using the selenium library. It sets up a web browser, navigates to the climate risk page, extracts data from an HTML table, and saves the scraped information into a CSV file named \"climate_risk_table.csv\". This file is then used in this jupyter notebook for analysis and visualization. The CSV file has also been provided for direct use.\n",
    "\n",
    "### Notebook Information\n",
    "<b>Climate Dashboard.ipynb</b>: This is the main jupyter notebook containing the analysis and visualization of the dashboard. There are four code blocks to execute. The details about their significance and the functions they contain are given squentially below:\n",
    "   > <b>Code Block 1</b>: All the required libraries (mentioned in the user guide) are imported to the program. The scrapped CSV file is then pulled and cleaned using various pandas function to ensure there are no irregularities. This includes splitting the county names into state abbreviations and county columns and creating unqiue identifiers for each row ('ID' column) in the <i>df</i> dataframe. This also creates five risk categories using the 'Score' column. We also pull the spatial data from the shapefile \"cb_2020_us_county_500k.shp\" and convert it to a usable dataframe <i>map</i> using geopandas. Using the state abbreviation and county names, we create a unique identifier ('ID' column) that is consistent with the unique identifiers of <i>df</i>. The two dataframes are then merged into a new dataframe called <i>mergeMap</i>. We also pull the latitude and longitude values from \"us-county-boundaries.csv\" into the <i>latlong</i> dataframe.\\\n",
    "    \\\n",
    "   > <b>Code Block 2</b>: This code block contains the <i>temppredict</i> function which uses the openmeteo api and pulls various historical climate data for the user-selected county using the latitude and longitude values as parameters from the <i>latlong</i> dataframe. The historical data are then analysed using multiple linear regression to predict the future temperature data for the selected county. The function returns the historical data, predicted data, combined index and trendline obtained from the regression.\\\n",
    "\\\n",
    "    > <b>Code Block 3</b>: This code block contains the <i>aqitrend</i> function which uses the openmeteo api and pulls the live air quality data for the user-selected county using the latitude and longitude values as parameters from the <i>latlong</i> dataframe. The function then returns the daily average data for PM10 and PM2.5 levels (as dataframe) along with their respective dates for tick values in the chart.\\\n",
    "\\\n",
    "    > <b> Code Block 4 </b>: This code block is used to build and execute the dashboard for our visualization. Using the dash library, we build a layout of the dashboard using HTML elements and create spaces for 5 charts to be displayed along with some markdown text. @app.callback is a callable functionality is used to specify the user's input and make interactive output elements. Following this, they are updated using update functions. We first create a dropdown user interface where the user can select the state and the county to observe using the <i>update_counties</i> function. This data is then is used to create another callback for data visualization which utilizes the update_charts function. This function uses the state and county inputs as parameters. Its full purpose is given below:\n",
    "    >> <u>Markdown</u>: From the user input, it uses markdown text to provide initial details about the county using markdown language using the risk score data. The percentile and risk categories are calculated here before returning the text. It is returned as \"markdown_content\".\\\n",
    "    >> <u>Bar Chart</u>: The bar chart contains the weighted risk score from for all the counties in the state along with the national and state mean values for comparison. The selected county is highlighted. In the code this is returned as \"fig\".\\\n",
    "    >> <u>Pie Chart</u>: This shows how the risk is distributed among different climate indicators. It is returned as \"figpie\".\\\n",
    "    >> <u>Choropleth Map</u>: This map contains the spatial data of the counties in the selected state along with the selected county highlighted with a bold outline. The 'Score' column (displayed in the legend) is used to determine the color of the marker for each county. It is returned as \"fig2\".\\\n",
    "    >> <u>Temperature Prediction Chart</u>: This calls the <i>temppredict</i> function that yields the historical and predicted data along with the trendline. The data is visualized here after conversion to the Fahrenheit scale. It is returned as \"fig3\".\\\n",
    "    >> <u>Air Quality Chart</u>: This calls the <i>aqitrend</i> function that yields the live average inhalable pariculate matter data in PM-10 and PM-2.5 levels and extends upto the last three months. The chart is displayed as a subplot containing two line charts placed vertically over the date to show the trend along with the safe levels. It is returned as \"fig4\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e49b86-db2b-4baf-9c15-a03cb54659ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "df = pd.read_csv('climate_risk_table.csv')\n",
    "df['County_State'] = df['County'].str.split(', ')\n",
    "county_names = df['County_State'].tolist()\n",
    "counties = []\n",
    "states = []\n",
    "for i in county_names:\n",
    "    counties.append(i[0])\n",
    "    states.append(i[1])\n",
    "df['County'] = counties\n",
    "df['State'] = states\n",
    "\n",
    "#Clean the scrapped file\n",
    "df['County']= df['County'].str.replace(' County','')\n",
    "df = df.sort_values(by=['State', 'County'])\n",
    "df['County']= df['County'].str.replace(' Parish','')\n",
    "df['County']= df['County'].str.replace('Ã±','ñ')\n",
    "\n",
    "#Attribute the risk categories\n",
    "conditions = [\n",
    "    (df['Score'] <= 7),\n",
    "    (df['Score'] > 7) & (df['Score'] <= 14),\n",
    "    (df['Score'] > 14) & (df['Score'] <= 21),\n",
    "    (df['Score'] > 21) & (df['Score'] <= 28),\n",
    "    (df['Score'] > 28)\n",
    "]\n",
    "\n",
    "choices = ['lowest risk', 'low risk', 'medium risk', 'high risk', 'highest risk']\n",
    "df['Risk'] = np.select(conditions, choices)\n",
    "\n",
    "\n",
    "#Create unique identifiers to merge with spatial data\n",
    "df['ID'] = df['State'] + '_' + df['County']\n",
    "df = df.drop(['County_State'], axis=1)\n",
    "\n",
    "\n",
    "#Load spatial data and merge with the file\n",
    "map = gpd.read_file('cb_2020_us_county_500k/cb_2020_us_county_500k.shp')\n",
    "map.loc[map['NAMELSAD'].str.endswith(' city'), 'NAME'] = map['NAMELSAD']\n",
    "map['ID']= map['STUSPS'] + '_' + map['NAME']\n",
    "\n",
    "mergeMap = pd.merge(map, df, on='ID',how='left')\n",
    "mergeMap = mergeMap.sort_values('ID')\n",
    "mergeMap = mergeMap.set_index('GEOID')\n",
    "\n",
    "#Remove areas for which data is not available\n",
    "no_loc = ['Alaska','Puerto Rico','Hawaii','United States Virgin Islands', 'American Samoa', 'Commonwealth of the Northern Mariana Islands']\n",
    "mergeMap = mergeMap[~mergeMap['STATE_NAME'].isin(no_loc)]\n",
    "\n",
    "#Get latitude and longitude details\n",
    "temp = gpd.read_file('us-county-boundaries.csv')\n",
    "latlong = temp[['GEOID','INTPTLAT','INTPTLON']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1d259f-350e-4078-a81b-d5b4162689d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Temperature Prediction from Open-Meteo API\n",
    "def temppredict(lat, lng):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "    \n",
    "    # Make sure all required weather variables are listed here\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "    \t\"latitude\": lat,\n",
    "    \t\"longitude\": lng,\n",
    "    \t\"start_date\": \"2013-01-01\",\n",
    "    \t\"end_date\": \"2023-12-31\",\n",
    "    \t\"daily\": [\"weather_code\", \"temperature_2m_max\", \"temperature_2m_min\", \"temperature_2m_mean\", \"daylight_duration\", \"sunshine_duration\", \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\", \"precipitation_hours\", \"wind_speed_10m_max\", \"wind_gusts_10m_max\", \"wind_direction_10m_dominant\", \"shortwave_radiation_sum\", \"et0_fao_evapotranspiration\"]\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    \n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "    \n",
    "    # Process daily data. The order of variables needs to be the same as requested.\n",
    "    daily = response.Daily()\n",
    "    daily_weather_code = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_temperature_2m_max = daily.Variables(1).ValuesAsNumpy()\n",
    "    daily_temperature_2m_min = daily.Variables(2).ValuesAsNumpy()\n",
    "    daily_temperature_2m_mean = daily.Variables(3).ValuesAsNumpy()\n",
    "    daily_daylight_duration = daily.Variables(4).ValuesAsNumpy()\n",
    "    daily_sunshine_duration = daily.Variables(5).ValuesAsNumpy()\n",
    "    daily_precipitation_sum = daily.Variables(6).ValuesAsNumpy()\n",
    "    daily_rain_sum = daily.Variables(7).ValuesAsNumpy()\n",
    "    daily_snowfall_sum = daily.Variables(8).ValuesAsNumpy()\n",
    "    daily_precipitation_hours = daily.Variables(9).ValuesAsNumpy()\n",
    "    daily_wind_speed_10m_max = daily.Variables(10).ValuesAsNumpy()\n",
    "    daily_wind_gusts_10m_max = daily.Variables(11).ValuesAsNumpy()\n",
    "    daily_wind_direction_10m_dominant = daily.Variables(12).ValuesAsNumpy()\n",
    "    daily_shortwave_radiation_sum = daily.Variables(13).ValuesAsNumpy()\n",
    "    daily_et0_fao_evapotranspiration = daily.Variables(14).ValuesAsNumpy()\n",
    "    \n",
    "    daily_data = {\"date\": pd.date_range(\n",
    "    \tstart = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "    \tend = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "    \tfreq = pd.Timedelta(seconds = daily.Interval()),\n",
    "    \tinclusive = \"left\"\n",
    "    )}\n",
    "    daily_data[\"weather_code\"] = daily_weather_code\n",
    "    daily_data[\"temperature_2m_max\"] = daily_temperature_2m_max\n",
    "    daily_data[\"temperature_2m_min\"] = daily_temperature_2m_min\n",
    "    daily_data[\"temperature_2m_mean\"] = daily_temperature_2m_mean\n",
    "    daily_data[\"daylight_duration\"] = daily_daylight_duration\n",
    "    daily_data[\"sunshine_duration\"] = daily_sunshine_duration\n",
    "    daily_data[\"precipitation_sum\"] = daily_precipitation_sum\n",
    "    daily_data[\"rain_sum\"] = daily_rain_sum\n",
    "    daily_data[\"snowfall_sum\"] = daily_snowfall_sum\n",
    "    daily_data[\"precipitation_hours\"] = daily_precipitation_hours\n",
    "    daily_data[\"wind_speed_10m_max\"] = daily_wind_speed_10m_max\n",
    "    daily_data[\"wind_gusts_10m_max\"] = daily_wind_gusts_10m_max\n",
    "    daily_data[\"wind_direction_10m_dominant\"] = daily_wind_direction_10m_dominant\n",
    "    daily_data[\"shortwave_radiation_sum\"] = daily_shortwave_radiation_sum\n",
    "    daily_data[\"et0_fao_evapotranspiration\"] = daily_et0_fao_evapotranspiration\n",
    "    \n",
    "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "    # Assuming df is your original DataFrame\n",
    "    daily_dataframe['month'] = daily_dataframe['date'].dt.to_period('M')  # Create a 'month' column based on the date\n",
    "    \n",
    "    # Dropping 'date' and 'weather_code' from the calculation\n",
    "    monthly_avg = daily_dataframe.groupby('month').mean().drop(columns=['weather_code'])\n",
    "    monthly_avg = monthly_avg.drop(columns=['date'])\n",
    "    \n",
    "    # Assume 'temperature_2m_mean' is the target variable and the others are features\n",
    "    X = monthly_avg.drop(columns=['temperature_2m_mean'])  # Independent variables\n",
    "    y = monthly_avg['temperature_2m_mean']  # Dependent variable\n",
    "    \n",
    "    # Check for missing values and handle them if needed\n",
    "    monthly_avg.fillna(monthly_avg.mean(), inplace=True)\n",
    "    \n",
    "    # Convert any non-numeric columns (if needed)\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the linear regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model using mean squared error or R-squared\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    \n",
    "    # Assuming you have future data to predict for the next 60 months, create that DataFrame\n",
    "    # For simplicity, we simulate this future data with placeholders (you'd need real or averaged data)\n",
    "    future_data = X.tail(60)  # Just as a placeholder, use actual values for future months\n",
    "    \n",
    "    # Predict for the next 60 months\n",
    "    future_predictions = model.predict(future_data)\n",
    "    \n",
    "    # Convert predictions to a DataFrame or Series\n",
    "    future_predictions_df = pd.DataFrame(future_predictions, columns=['predicted_temperature_2m_mean'])\n",
    "\n",
    "    # Assuming your DataFrame is called monthly_avg and 'month' is the index\n",
    "    # Convert the PeriodIndex to DatetimeIndex\n",
    "    monthly_avg.index = monthly_avg.index.to_timestamp()\n",
    "    \n",
    "    # Assuming your original DataFrame is called monthly_avg and 'month' is the index\n",
    "    # Convert the index to datetime if necessary\n",
    "    if not isinstance(monthly_avg.index, pd.DatetimeIndex):\n",
    "        monthly_avg.index = pd.to_datetime(monthly_avg.index, format='%Y-%m')\n",
    "    \n",
    "    # Assuming the new predicted DataFrame is called future_predictions_df, and it contains 60 months of predictions\n",
    "    # Create a date range for the next 60 months based on the last date in monthly_avg\n",
    "    future_dates = pd.date_range(start=monthly_avg.index[-1] + pd.DateOffset(months=1), periods=60, freq='M')\n",
    "    future_predictions_df.index = future_dates\n",
    "    \n",
    "    # Combine the indices of both historical and future data for the trend line\n",
    "    combined_index = monthly_avg.index.append(future_predictions_df.index)\n",
    "    \n",
    "    # Convert both historical and future dates to ordinal (numerical) format for the trend line\n",
    "    x_combined = combined_index.map(pd.Timestamp.toordinal).values\n",
    "    y_combined = np.concatenate((monthly_avg['temperature_2m_mean'].values, future_predictions_df['predicted_temperature_2m_mean'].values))\n",
    "    \n",
    "    # Fitting a linear regression to get the extended trend line\n",
    "    coefficients = np.polyfit(x_combined, y_combined, 1)\n",
    "    extended_trendline = np.polyval(coefficients, x_combined)\n",
    "    return monthly_avg, future_predictions_df, combined_index, extended_trendline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4367dbc3-2bf8-4fcc-85d7-164a1ec376fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Live AQI Trend\n",
    "def aqitrend(lat, lng):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "    \t\"latitude\": lat,\n",
    "    \t\"longitude\": lng,\n",
    "    \t\"hourly\": [\"pm10\", \"pm2_5\"],\n",
    "    \t\"past_days\": 92\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    \n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "    print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "    print(f\"Elevation {response.Elevation()} m asl\")\n",
    "    print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "    print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "    \n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = response.Hourly()\n",
    "    hourly_pm10 = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_pm2_5 = hourly.Variables(1).ValuesAsNumpy()\n",
    "    \n",
    "    hourly_data = {\"date\": pd.date_range(\n",
    "    \tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "    \tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "    \tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "    \tinclusive = \"left\"\n",
    "    )}\n",
    "    hourly_data[\"pm10\"] = hourly_pm10\n",
    "    hourly_data[\"pm2_5\"] = hourly_pm2_5\n",
    "    \n",
    "    hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "    print(hourly_dataframe)\n",
    "        \n",
    "    # Step 1: Convert the 'date' column to datetime if it isn't already\n",
    "    hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])\n",
    "    \n",
    "    # Step 2: Create a new column 'date_only' to store only the date (without hours)\n",
    "    hourly_dataframe['date_only'] = hourly_dataframe['date'].dt.date\n",
    "    \n",
    "    # Step 3: Group by 'date_only' and calculate the daily average of 'pm10' and 'pm2_5'\n",
    "    daily_avg = hourly_dataframe.groupby('date_only').mean().reset_index()\n",
    "    \n",
    "    # Step 4: Convert 'date_only' back to datetime for plotting\n",
    "    daily_avg['date_only'] = pd.to_datetime(daily_avg['date_only'])\n",
    "    \n",
    "    # Step 5: Get the min and max values of the 'date_only' column for the x-axis limits\n",
    "    min_date = daily_avg['date_only'].min()\n",
    "    max_date = daily_avg['date_only'].max()\n",
    "    \n",
    "    # Step 6: Get the maximum values of pm10 and pm2_5 and add 10 to them\n",
    "    max_pm10 = daily_avg['pm10'].max() + 5\n",
    "    max_pm2_5 = daily_avg['pm2_5'].max() + 5\n",
    "    return daily_avg, min_date, max_date, max_pm10, max_pm2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84065193-4a8e-40e6-a684-ee7c71974161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x13fd9c9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize and run the web-based interface\n",
    "app = Dash(__name__)\n",
    "\n",
    "#Define the structure and appearance of the application using HTML and Dash components.\n",
    "app.layout = html.Div([\n",
    "    dcc.Dropdown(\n",
    "        id='state-dropdown',\n",
    "        options=[{'label': state, 'value': state} for state in mergeMap['STATE_NAME'].unique()],\n",
    "        value='Pennsylvania'\n",
    "    ),\n",
    "    dcc.Dropdown(id='county-dropdown'),\n",
    "    dcc.Markdown(id='county-info'),\n",
    "    dcc.Graph(id='score-chart'),\n",
    "    dcc.Graph(id='factors-pie'),\n",
    "    dcc.Graph(id='choropleth-map'),\n",
    "    dcc.Graph(id='timeseries'),\n",
    "    dcc.Graph(id='aqi'),\n",
    "])\n",
    "\n",
    "#Update parts of the application in response to user interactions\n",
    "@app.callback(\n",
    "    Output('county-dropdown', 'options'),\n",
    "    Output('county-dropdown', 'value'),\n",
    "    Input('state-dropdown', 'value')\n",
    ")\n",
    "\n",
    "#Update the counties based on user selection\n",
    "\n",
    "def update_counties(selected_state):\n",
    "    filtered_df = mergeMap[mergeMap['STATE_NAME'] == selected_state]\n",
    "    options = [{'label': county, 'value': county} for county in filtered_df['County']]\n",
    "    return options, options[0]['value']\n",
    "\n",
    "@app.callback(\n",
    "    Output('county-info', 'children'),\n",
    "    Output('score-chart', 'figure'),\n",
    "    Output('factors-pie','figure'),\n",
    "    Output('choropleth-map', 'figure'),\n",
    "    Output('timeseries', 'figure'),\n",
    "    Output('aqi', 'figure'),\n",
    "    Input('state-dropdown', 'value'),\n",
    "    Input('county-dropdown', 'value'),\n",
    ")\n",
    "\n",
    "# Create and return charts for visualization\n",
    "def update_charts(selected_state, selected_county):\n",
    "    viz_df = mergeMap[mergeMap['STATE_NAME'] == selected_state] #Filter dataframe\n",
    "    bar_df = viz_df.drop_duplicates(subset='ID', keep='first')\n",
    "    bar_df = viz_df.sort_values(by=['STATE_NAME', 'Score', 'County'])\n",
    "    colors = ['#fee08b' if county != selected_county else '#fc8d59' for county in bar_df['County']]\n",
    "    nat_mean = mergeMap['Score'].mean()\n",
    "    state_mean = bar_df['Score'].mean()\n",
    "    \n",
    "    # Bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=bar_df['County'],\n",
    "            y=bar_df['Score'].astype(int),\n",
    "            marker_color=colors,\n",
    "            hovertemplate='Risk of %{x}: %{y}<extra></extra>'\n",
    "        )\n",
    "    ])\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bar_df['County'],\n",
    "        y=[nat_mean] * len(bar_df),\n",
    "        mode='lines',\n",
    "        name='National Mean',\n",
    "        hovertemplate=f'<b>National Mean: {nat_mean:.2f}<extra></extra>',\n",
    "        line=dict(color='#d53e4f', dash='dash')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bar_df['County'],\n",
    "        y=[state_mean] * len(bar_df),\n",
    "        mode='lines',\n",
    "        name='State Mean',\n",
    "        hovertemplate=f'<b>State Mean: {state_mean:.2f}<extra></extra>',\n",
    "        line=dict(color='#3288bd', dash='dash')\n",
    "    ))\n",
    "    fig.update_xaxes(\n",
    "    showticklabels=False,\n",
    "    showgrid=False,\n",
    "    zeroline=False)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Risk Score of Counties in {selected_state}',\n",
    "        yaxis_title='Score',\n",
    "        xaxis_title = 'Counties',\n",
    "        height=500,\n",
    "        showlegend = False,\n",
    "        paper_bgcolor = \"rgba(0, 0, 0, 0)\",\n",
    "        plot_bgcolor = \"rgba(0, 0, 0, 0)\"\n",
    "    )\n",
    "\n",
    "    #Risk Factors Pie chart\n",
    "    county_df = viz_df[viz_df['County'] == selected_county]\n",
    "    colors = ['#fc8d59','#e6f598','#fee08b','#3288bd','#d53e4f','#99d594']\n",
    "    piecols = ['Heat', 'Wet Bulb', 'Farm Crop Yields', 'Sea Level Rise','Very Large Fires', 'Economic Damages']\n",
    "    pievals = county_df[piecols].sum()\n",
    "    figpie = go.Figure(data=[go.Pie(labels=piecols, values=pievals)])\n",
    "    figpie.update_traces(hoverinfo='label+value', texttemplate=\"<b>%{label}</b><br>%{percent}\", textfont_size=12, textfont_color = \"black\",\n",
    "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)),showlegend=False)\n",
    "    figpie.update_layout(title=f'Distribution of Risk Factors for {selected_county} County',margin={\"r\": 250, \"t\": 50, \"l\": 50, \"b\": 50})\n",
    "    \n",
    "    #Choropleth Map\n",
    "    \n",
    "    fig2 = go.Figure(data=go.Choropleth(\n",
    "        geojson = viz_df.__geo_interface__,\n",
    "        locations=viz_df.index,\n",
    "        z = viz_df['Score'].astype(float),\n",
    "        zmin = 0,\n",
    "        zmax = 35,\n",
    "        text = viz_df['NAME'],\n",
    "        colorscale = 'YlOrRd',\n",
    "        colorbar_title = \"Risk Score\",\n",
    "        marker_line_color='black',\n",
    "        marker_line_width=0.5,\n",
    "        customdata=viz_df[['County', 'Score']]\n",
    "    ))\n",
    "    fig2.update_traces(\n",
    "        hovertemplate=\"<b>%{customdata[0]}</b><br>Risk Score: %{customdata[1]:.0f}<extra></extra>\")\n",
    "    fig2.add_choropleth(\n",
    "        geojson = viz_df[viz_df['NAME'] == selected_county].__geo_interface__,\n",
    "        locations=viz_df[viz_df['NAME'] == selected_county].index,\n",
    "        z = [viz_df.loc[viz_df['NAME'] == selected_county]['Score'].values[0]],\n",
    "        zmin = 0,\n",
    "        zmax = 35,\n",
    "        text = selected_county,\n",
    "        colorscale = 'YlOrRd',\n",
    "        showscale=False,\n",
    "        marker_line_color='black',\n",
    "        marker_line_width=2,\n",
    "        customdata=[viz_df[viz_df['NAME'] == selected_county]['County'].values],\n",
    "        hovertemplate=\"<b>%{customdata[0]}</b><br>Risk Score: %{z}<extra></extra>\"\n",
    "    )\n",
    "    fig2.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    fig2.update_layout(\n",
    "        title_text = f'Spatial Risk Map of Counties in {selected_state}', margin={\"r\": 300, \"t\": 50, \"l\": 50, \"b\": 50})\n",
    "\n",
    "    viz_df = viz_df.join(latlong.set_index('GEOID'))\n",
    "    lat = viz_df.loc[viz_df['NAME'] == selected_county]['INTPTLAT'].values[0]\n",
    "    lng = viz_df.loc[viz_df['NAME'] == selected_county]['INTPTLON'].values[0]\n",
    "    \n",
    "    #Temperature Prediction using Machine Learning\n",
    "    monthly_avg, future_predictions_df, combined_index, extended_trendline = temppredict(lat,lng)\n",
    "\n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    # Plot the historical data\n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=monthly_avg.index,\n",
    "        y=(9/5)*monthly_avg['temperature_2m_mean']+32,\n",
    "        mode='lines+markers',\n",
    "        name='Mean Temperature (Historical)',\n",
    "        line=dict(color='#3288bd')\n",
    "    ))\n",
    "    \n",
    "    # Add the predicted temperature_2m_mean values\n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=future_predictions_df.index,\n",
    "        y=(9/5)*future_predictions_df['predicted_temperature_2m_mean']+32,\n",
    "        mode='lines+markers',\n",
    "        name='Predicted Mean Temperature',\n",
    "        line=dict(color='#99d594')\n",
    "    ))\n",
    "    \n",
    "    # Plot the extended trendline\n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=combined_index,\n",
    "        y=(9/5)*extended_trendline+32,\n",
    "        mode='lines',\n",
    "        name='Extended Trend Line',\n",
    "        line=dict(color='#d53e4f')\n",
    "    ))\n",
    "    \n",
    "\n",
    "    fig3.update_layout(\n",
    "        title=f'Temperature Prediction for Next 5 Years for {selected_county} county, {selected_state}',\n",
    "        xaxis_title='Time (Year)',\n",
    "        yaxis_title='Temperature Mean (°F)',\n",
    "        xaxis=dict(tickangle=45),\n",
    "        legend=dict(\n",
    "        x=1.05,  \n",
    "        y=1,\n",
    "        traceorder='normal',\n",
    "        orientation='v'\n",
    "    ),\n",
    "        margin={\"r\": 300, \"t\": 50, \"l\": 50, \"b\": 50},\n",
    "        template='plotly_white',\n",
    "        paper_bgcolor = \"rgba(0, 0, 0, 0)\",\n",
    "        plot_bgcolor = \"rgba(0, 0, 0, 0)\"\n",
    "    )    \n",
    "\n",
    "    #AQI Trendline\n",
    "    daily_avg,min_date,max_date, max_pm10, max_pm2_5 = aqitrend(lat,lng)\n",
    "    \n",
    "    # Create subplots: 2 rows, 1 column\n",
    "    fig4 = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "    \n",
    "    fig4.add_trace(\n",
    "        go.Scatter(\n",
    "            x=daily_avg['date_only'],\n",
    "            y=daily_avg['pm10'],\n",
    "            mode='lines',\n",
    "            name='PM10 Levels',\n",
    "            line=dict(color='#3288bd')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig4.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_date, max_date],\n",
    "            y=[15, 15],\n",
    "            mode='lines',\n",
    "            name='Safe Average Annual Level',\n",
    "            line=dict(color='#d53e4f', dash='dash', width=2.5)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig4.add_trace(\n",
    "        go.Scatter(\n",
    "            x=daily_avg['date_only'],\n",
    "            y=daily_avg['pm2_5'],\n",
    "            mode='lines',\n",
    "            name='PM2.5 Levels',\n",
    "            line=dict(color='#fc8d59')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig4.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_date, max_date],\n",
    "            y=[5, 5],\n",
    "            mode='lines',\n",
    "            name='Safe Average Annual Level',\n",
    "            line=dict(color='#d53e4f', dash='dash', width=2.5),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig4.update_layout(\n",
    "        template='plotly_white',\n",
    "        title_text='Average Daily Inhalable Particulate Matter Over Last 3 Months',\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        paper_bgcolor = \"rgba(0, 0, 0, 0)\",\n",
    "        plot_bgcolor = \"rgba(0, 0, 0, 0)\",\n",
    "        margin={\"r\": 300, \"t\": 75, \"l\": 50, \"b\": 50}\n",
    "    )\n",
    "    \n",
    "    fig4.update_yaxes(title_text=\"PM10 Levels\", range=[0, max_pm10], row=1, col=1)\n",
    "    fig4.update_yaxes(title_text=\"PM2.5 Levels\", range=[0, max_pm2_5], row=2, col=1)\n",
    "    fig4.update_xaxes(title_text=\"Time\", range=[min_date, max_date], row=2, col=1)\n",
    "\n",
    "    #Percentile and Risk Calculation\n",
    "    county_score = viz_df.loc[mergeMap['County'] == selected_county]['Score'].values[0]\n",
    "    score_list = mergeMap['Score'].tolist()\n",
    "    score_list = [int(score) if not np.isnan(score) else 0 for score in score_list]\n",
    "    percentile = percentileofscore(score_list, county_score)\n",
    "    percentile = round(percentile, 0)\n",
    "    getgeoid = viz_df.loc[mergeMap['County'] == selected_county].index\n",
    "    risk_level = mergeMap.loc[getgeoid, 'Risk'].values[0]\n",
    "    markdown_content= f'''\n",
    "        ### Climate Risk and Impact Assessment Dashboard for US Counties\n",
    "        {selected_county} county is in percentile **{int(percentile)}** of all national counties for total climate risk.\\n\n",
    "        {selected_county} county is considered to be *{risk_level}*.\\n \n",
    "    '''\n",
    "\n",
    "    return markdown_content, fig, figpie, fig2, fig3, fig4\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b7cfc-db9c-4e28-9a81-c3ea928a50a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
